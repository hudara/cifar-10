{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CIFAR-10 Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this document I will explain in details my approach of solving the CIFAR-10 challenge.\n",
    "I will describe in details the problem, the solution, and describe the way I choose the different parameters.\n",
    "I will begin with the bottom line: Using my own personal laptop machine which is a Window-7 64-bit with 8GB of RAM, and Intel Core i5-5300U CPU of 2.30 GHZ, I have successfully matched **70.05%** of the images in about 53 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "CIFAR-10 is a dataset consist of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The test images are composed of 1000 randomly selected images from each class. The training images contains 5,000 images from each class in random order.\n",
    "The goal of this competition, is to build an algorithm which takes a random image and classify it to one of the 10 classes with as high as possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Basically we are talking about a classifier algorithm. This algorithm should use the training images to build a classifier model, and then apply the test images on this model to test how good this model really is.\n",
    "The following chart shows the building block of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Phase\n",
    "1. **Training Image:** The algorithm start with a set of 50,000 classified training images.\n",
    "2. **Preprocessing:** These images are send to the preprocessing phase, in which they are prepared for the next phases. For example cleaning the images from noise, so that the noise will not affect the next phases.\n",
    "3. **Feature Extraction:** Now, each image is ready for feature extraction phase, in which it is represented as a vector\n",
    "4. **Dimensionality Reduction:** Once the images are converted into vectors, the algorithm reduce the dimension of these vectors. This dimension reduction is done under the assumption that similar images (from the same class) will have similar (or close) vectors.\n",
    "5. **Learning Algorithm:** Now, these reduced vectors are ready to be sent to the learning algorithm, which receives these vectors of images a long with the label of each image, and calculate the classifier model. This model will be used later for the test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Phase\n",
    "Once the model classifier is ready, it can used by the test images. \n",
    "These test images are going through the same pipeline of: preprocessing, feature extraction, and dimension reduction. \n",
    "Then they are ready to be sent to the model classifier created in phase 5 above, to be classified. \n",
    "The solution is than compared to the real classes of the test images and the accuracy level and confusion matrix are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "## Import the relevant libraries\n",
    "## ==============================\n",
    "import numpy\n",
    "import time\n",
    "import calendar\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "## General parameters:\n",
    "## ===================\n",
    "imageSize = 32  #The size of the original image - in pixels - assuming this is a square image\n",
    "channels = 3    #The number of channels of the image. A RBG color image, has 3 channels\n",
    "classes = 10    #The number of classes available for this dataset\n",
    "trainingDataSize = 50000    #The number of images in the training set\n",
    "testDataSize = 10000        #The number of images in the test set\n",
    "trainigDataFiles = ('./dataset/data_batch_1', './dataset/data_batch_2', './dataset/data_batch_3', './dataset/data_batch_4','./dataset/data_batch_5') #An array of filenames containing the training data set\n",
    "testDataFile = './dataset/test_batch' #The filename containing the test set\n",
    "pcaFileName = 'pca' #The PCS filename\n",
    "svmFileName = 'svm' #The SVM filename\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HOG Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "## HOG Parameters:\n",
    "## ================\n",
    "winSize = imageSize\n",
    "blockSize = 12\n",
    "blockStride = 4\n",
    "cellSize = 4\n",
    "nbins = 18\n",
    "derivAperture = 1\n",
    "winSigma = -1.\n",
    "histogramNormType = 0\n",
    "L2HysThreshold = 0.2\n",
    "gammaCorrection = True\n",
    "nlevels = 64\n",
    "signedGradient = True\n",
    "hog = cv2.HOGDescriptor((winSize,winSize),(blockSize, blockSize),(blockStride,blockStride),(cellSize,cellSize),nbins,derivAperture, winSigma,histogramNormType,L2HysThreshold,gammaCorrection,nlevels,signedGradient)\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "##SVM parameters:\n",
    "##===============\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setType(cv2.ml.SVM_C_SVC)\n",
    "svm.setKernel(cv2.ml.SVM_RBF)\n",
    "svm.setC(0.1)\n",
    "svm.setGamma(0.1)\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "##PCA parameters:\n",
    "##==================\n",
    "pcaDim = 3000\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data from the given filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "def loadData(filename):\n",
    "    '''\n",
    "    Load the data from the given filename\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename: string\n",
    "        The name of the file containing the data to load\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    theSet['data']:     array of images\n",
    "    theSet['labels']:   array of labels\n",
    "    '''\n",
    "    f = open(filename, 'rb')\n",
    "    theSet = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return theSet['data'], theSet['labels']\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the images from CIFAR-10 format, to an array of 10000 images each is 32 X 32 X 3 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "def convertImages(origImages):\n",
    "    '''\n",
    "    Convert the images from CIFAR-10 format, to an array of 10000 images each is 32 X 32 X 3 size\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    origImages: array\n",
    "        array of images in the CIFAR-10 format\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    images:     array of images each in 32 X 32 X 3 size\n",
    "    '''\n",
    "    images = np.reshape(origImages,(-1, channels, imageSize, imageSize))\n",
    "    images = np.transpose(images, (0,2,3,1))\n",
    "\n",
    "    return images\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "def loadTestData(filename):\n",
    "    '''\n",
    "    Load the test data\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename: string\n",
    "        The name of the file containing the test data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    testImages: array of images of the test data\n",
    "    testLabels: array of labels of the test data\n",
    "    '''\n",
    "\n",
    "    origTestImages, testLabels = loadData(filename)\n",
    "    testImages = convertImages(origTestImages)\n",
    "\n",
    "    return testImages, testLabels\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load all the training data from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "def loadTrainingData(filenames):\n",
    "    '''\n",
    "    Load all the training data from all files\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filenames: array of string\n",
    "        An array The name of the file containing the data to load\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    trainingImages: array of the training set images\n",
    "    trainingLabels: array of the training set labels\n",
    "    '''\n",
    "\n",
    "    #Pre-allocate the arrays\n",
    "    trainingImages = np.zeros(shape=[trainingDataSize, imageSize, imageSize, channels], dtype=numpy.uint8)\n",
    "    trainingLabels = np.zeros(shape=[trainingDataSize], dtype=int)\n",
    "\n",
    "    start=0\n",
    "    for fileName in filenames:\n",
    "        origImages, labels = loadData(fileName)\n",
    "        images = convertImages(origImages)\n",
    "\n",
    "        numOfImages = len(images);\n",
    "        end = start + numOfImages;\n",
    "        trainingImages[start:end, :] = images\n",
    "        trainingLabels[start:end] = labels\n",
    "        start = end\n",
    "\n",
    "    return trainingImages, trainingLabels\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Returns the current time in seconds since EPOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "def currentTime():\n",
    "    '''\n",
    "    Returns the current time in seconds since EPOC\n",
    "    Used to measure how much time each phase took\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    the current time in second since EPOC\n",
    "    '''\n",
    "\n",
    "    return calendar.timegm(time.gmtime())\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the HOG descriptors of the given images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "def calcHOG(images):\n",
    "    '''\n",
    "    Calculate the HOG descriptors of the given images\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    images: an array of images\n",
    "        The images to which a HOG calculation should be applied\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    hogDescriptors: an array of HOG vectors, 5832 components each\n",
    "    '''\n",
    "\n",
    "    hogDescriptors = []\n",
    "    for image in images:\n",
    "        hogDescriptors.append( hog.compute(image) )\n",
    "\n",
    "    hogDescriptors = np.squeeze(hogDescriptors)\n",
    "    return hogDescriptors\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First load the data into two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training set... Took: 1 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the training set...\"),\n",
    "tik = currentTime()\n",
    "trainingImages, trainingLabels = loadTrainingData(trainigDataFiles)\n",
    "print(\"Took: \" + str(currentTime()-tik) + \" sec\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create a HOG descriptor from these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HOG descriptors from the training set... Took: 15 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating HOG descriptors from the training set...\"),\n",
    "tik = currentTime()\n",
    "trainHogDescriptors = calcHOG(trainingImages)\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce the dimension of the HOG descriptors to 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the dimension of the HOG descriptors to 3000... Took: 606 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reducing the dimension of the HOG descriptors to \" + str(pcaDim) + \"...\"),\n",
    "tik = currentTime()\n",
    "pca = PCA(pcaDim)\n",
    "trainHogProjected = pca.fit_transform(trainHogDescriptors)\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save it as a pca file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save it as a PCA file... Took: 8 sec\n"
     ]
    }
   ],
   "source": [
    "    print(\"Save it as a PCA file...\"),\n",
    "    tik = currentTime()\n",
    "    pcaFile = open(pcaFileName, 'wb')\n",
    "    pickle.dump(pca, pcaFile)\n",
    "    pcaFile.close()\n",
    "    print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the SVM model using the reduced HOG descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM model using the reduced HOG descriptor... Took: 2418 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the SVM model using the reduced HOG descriptor...\"),\n",
    "tik = currentTime()\n",
    "svm.train(np.asarray(trainHogProjected), cv2.ml.ROW_SAMPLE, np.asarray(trainingLabels))\n",
    "svm.save(svmFileName)\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the test set... Took: 1 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the test set...\"),\n",
    "tik = currentTime()\n",
    "testImages, testLabels = loadTestData(testDataFile)\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create HOG descriptors from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HOG descriptors from the test set... Took: 3 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating HOG descriptors from the test set...\"),\n",
    "tik = currentTime()\n",
    "testHogDescriptors = calcHOG(testImages);\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce the dimension of the HOG descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the dimension of the HOG descriptors to 3000... Took: 6 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reducing the dimension of the HOG descriptors to \" + str(pcaDim) + \"...\"),\n",
    "tik = currentTime()\n",
    "testHogProjected = pca.transform(testHogDescriptors)\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classify the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying the test set... Took: 419 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifying the test set...\"),\n",
    "tik = currentTime()\n",
    "testResponse = svm.predict(np.asarray(testHogProjected))[1].ravel()\n",
    "print(\"Took: \" + str(currentTime() - tik) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "=================\n",
      "[[761   9  68  19  27   8  13   3  70  22]\n",
      " [ 21 827   8  13  12   4  19   2  42  52]\n",
      " [ 69   8 591  80  88  70  42  18  27   7]\n",
      " [ 34  11 120 527  76 141  42  21  15  13]\n",
      " [ 27  11  79  94 657  24  41  46  17   4]\n",
      " [ 16   9  85 220  68 518  24  48   7   5]\n",
      " [ 22  18  44  71  34  32 764   4   9   2]\n",
      " [ 14   4  38  67  73  60   9 709  11  15]\n",
      " [ 51  24  22  17  11  10   6   6 833  20]\n",
      " [ 32  47  11  22  16   8   7  13  26 818]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Confusion matrix:\")\n",
    "print (\"=================\")\n",
    "confusionMatrix = confusion_matrix(testLabels, testResponse)\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Percentage Accuracy: 70.05 %\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "print (\"======================================\")\n",
    "accuracy = (np.asarray(testLabels) == testResponse).mean()\n",
    "print(\"Percentage Accuracy: %.2f %%\" % (accuracy*100))\n",
    "print (\"======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
